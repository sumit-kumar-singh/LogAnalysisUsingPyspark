{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Web Server Log Analysis with Apache Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 1 : Introduction and Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are doing analysis on NASA access log dataset for the month of July and August.\n",
    "\n",
    "\n",
    "* Folder Structure\n",
    "    - log-anaysis-using-pyspark\n",
    "        - dataset\n",
    "            - log_aug.gz\n",
    "            - log_jul.gz\n",
    "        - Notebook.ipynb\n",
    "        - DockerFile\n",
    "\n",
    "\n",
    "* log_jul.gz - contains logs for month of July\n",
    "* log_aug.gz - contains logs for month of August"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.11.29.55:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f32c3d0dfd0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "# SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Regular expressions\n",
    "import re\n",
    "\n",
    "# Pandas\n",
    "import pandas as pd\n",
    "\n",
    "# creating sparkSession\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"Log Analysis\") \\\n",
    "                    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "                    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP 2 : Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type :  <class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "# reading data as dataframe\n",
    "data = spark.read.format(\"csv\") \\\n",
    "               .option(\"inferSchema\", \"true\") \\\n",
    "               .option(\"header\",\"false\") \\\n",
    "               .load(\"dataset/*.gz\")\n",
    "print('Data type : ', type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows and columns : \n",
      "(3461613, 1)\n"
     ]
    }
   ],
   "source": [
    "# counting number of records\n",
    "print('Total rows and columns : ')\n",
    "print((data.count(), len(data.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- log: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renaming default column name\n",
    "data = data.withColumnRenamed(\"_c0\", \"log\") \n",
    "\n",
    "# viewing schema of dataset\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|log                                                                                                                    |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245                                 |\n",
      "|unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985                      |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085   |\n",
      "|burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0               |\n",
      "|199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179|\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing dataset\n",
    "data.show(5, truncate=False)\n",
    "\n",
    "# details about dataset : host_name, timestamp, request, response_code, data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing the log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample log data is : \n",
      " ['199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245', 'unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985', '199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085', 'burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0', '199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179', 'burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /images/NASA-logosmall.gif HTTP/1.0\" 304 0', 'burger.letters.com - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/video/livevideo.gif HTTP/1.0\" 200 0', '205.212.115.106 - - [01/Jul/1995:00:00:12 -0400] \"GET /shuttle/countdown/countdown.html HTTP/1.0\" 200 3985', 'd104.aa.net - - [01/Jul/1995:00:00:13 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985', '129.94.144.152 - - [01/Jul/1995:00:00:13 -0400] \"GET / HTTP/1.0\" 200 7074']\n"
     ]
    }
   ],
   "source": [
    "# getting sample log for operations\n",
    "sample_log_data = []\n",
    "for item in data.take(10):\n",
    "    sample_log_data.append(item[\"log\"])\n",
    "print('Sample log data is : \\n', sample_log_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching status code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['200', '200', '200', '304', '200', '304', '200', '200', '200', '200']\n"
     ]
    }
   ],
   "source": [
    "# regex pattern for status code\n",
    "# \\s matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "# capture group (\\d{3}) \\d{3} matches a digit (equal to [0-9]) {3} Quantifier — Matches exactly 3 times\n",
    "status_code_regx = r'\\s(\\d{3})\\s'\n",
    "status = []\n",
    "for item in sample_log_data:\n",
    "    status.append(re.search(status_code_regx, item).group(1))\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching content size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['6245', '3985', '4085', '0', '4179', '0', '0', '3985', '3985', '7074']\n"
     ]
    }
   ],
   "source": [
    "# regex pattern for content size\n",
    "# \\s matches any whitespace character (equal to [\\r\\n\\t\\f\\v ])\n",
    "# \\d+ matches a digit (equal to [0-9])\n",
    "# + Quantifier — Matches between one and unlimited times, as many times as possible\n",
    "# $ asserts position at the end of a line\n",
    "content_size_regx = r'\\s(\\d+)$'\n",
    "content_size = []\n",
    "for item in sample_log_data:\n",
    "    content_size.append(re.search(content_size_regx, item).group(1))\n",
    "print(content_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching host name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['199.72.81.55', 'unicomp6.unicomp.net', '199.120.110.21', 'burger.letters.com', '199.120.110.21', 'burger.letters.com', 'burger.letters.com', '205.212.115.106', 'd104.aa.net', '129.94.144.152']\n"
     ]
    }
   ],
   "source": [
    "# regex pattern for host name\n",
    "# 1st Capturing Group (^.[\\S+\\.]+\\S+)\n",
    "# ^ asserts position at start of a line . matches any character (except for line terminators)\n",
    "# Match a single character present in the list below [\\S+\\.]+\n",
    "# + Quantifier — Matches between one and unlimited times, as many times as possible\n",
    "host_name_regx = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "host_name = []\n",
    "for item in sample_log_data:\n",
    "    host_name.append(re.search(host_name_regx, item).group(1))\n",
    "print(host_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['01/Jul/1995:00:00:01 -0400', '01/Jul/1995:00:00:06 -0400', '01/Jul/1995:00:00:09 -0400', '01/Jul/1995:00:00:11 -0400', '01/Jul/1995:00:00:11 -0400', '01/Jul/1995:00:00:12 -0400', '01/Jul/1995:00:00:12 -0400', '01/Jul/1995:00:00:12 -0400', '01/Jul/1995:00:00:13 -0400', '01/Jul/1995:00:00:13 -0400']\n"
     ]
    }
   ],
   "source": [
    "# regex pattern for time stamp\n",
    "# 1st Capturing Group (\\d\\d/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})\n",
    "# \\d matches a digit (equal to [0-9])\n",
    "# \\w{3} matches any word character (equal to [a-zA-Z0-9_])\n",
    "# {3} Quantifier — Matches exactly 3 times\n",
    "# \\d{4} matches a digit (equal to [0-9])\n",
    "# {4} Quantifier — Matches exactly 4 times\n",
    "time_stamp_regx = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} -\\d{4})]'\n",
    "time_stamp = []\n",
    "for item in sample_log_data:\n",
    "    time_stamp.append(re.search(time_stamp_regx, item).group(1))\n",
    "print(time_stamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fetching methods, urls and protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('GET', '/history/apollo/', 'HTTP/1.0'), ('GET', '/shuttle/countdown/', 'HTTP/1.0'), ('GET', '/shuttle/missions/sts-73/mission-sts-73.html', 'HTTP/1.0'), ('GET', '/shuttle/countdown/liftoff.html', 'HTTP/1.0'), ('GET', '/shuttle/missions/sts-73/sts-73-patch-small.gif', 'HTTP/1.0'), ('GET', '/images/NASA-logosmall.gif', 'HTTP/1.0'), ('GET', '/shuttle/countdown/video/livevideo.gif', 'HTTP/1.0'), ('GET', '/shuttle/countdown/countdown.html', 'HTTP/1.0'), ('GET', '/shuttle/countdown/', 'HTTP/1.0'), ('GET', '/', 'HTTP/1.0')]\n"
     ]
    }
   ],
   "source": [
    "# regex pattern for combined methods, urls and protocol\n",
    "# 1st Capturing Group (\\S+) \\S+ matches any non-whitespace character (equal to [^\\r\\n\\t\\f\\v ]) \n",
    "# + Quantifier — Matches between one and unlimited times, as many times as possible\n",
    "# \\s matches any whitespace character \n",
    "# \\S* matches any non-whitespace character (equal to [^\\r\\n\\t\\f\\v ])\n",
    "methods_urls_protocol_regx = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "methods_urls_protocol = []\n",
    "for item in sample_log_data:\n",
    "    methods_urls_protocol.append(re.search(methods_urls_protocol_regx, item).groups())\n",
    "print(methods_urls_protocol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing regex_extract for extraction of regex based data\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "processed_data = data.select(regexp_extract('log', host_name_regx, 1).alias('host'),\n",
    "                            regexp_extract('log', time_stamp_regx, 1).alias('timestamp'),\n",
    "                            regexp_extract('log', methods_urls_protocol_regx, 1).alias('method'),\n",
    "                            regexp_extract('log', methods_urls_protocol_regx, 2).alias('endpoint'),\n",
    "                            regexp_extract('log', methods_urls_protocol_regx, 3).alias('protocol'),\n",
    "                            regexp_extract('log', status_code_regx, 1).cast('integer').alias('status'),\n",
    "                            regexp_extract('log', content_size_regx, 1).cast('integer').alias('content_size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|                host|           timestamp|method|            endpoint|protocol|status|content_size|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|   GET|    /history/apollo/|HTTP/1.0|   200|        6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   304|           0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|   GET|/shuttle/missions...|HTTP/1.0|   200|        4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/images/NASA-logo...|HTTP/1.0|   304|           0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|           0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|   GET|/shuttle/countdow...|HTTP/1.0|   200|        3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|   GET| /shuttle/countdown/|HTTP/1.0|   200|        3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|   GET|                   /|HTTP/1.0|   200|        7074|\n",
      "+--------------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing new dataframe \n",
    "processed_data.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- content_size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# schema of dataset\n",
    "processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows and columns : \n",
      "(3461613, 7)\n"
     ]
    }
   ],
   "source": [
    "# count of rows of new dataframe\n",
    "print('Total rows and columns : ')\n",
    "print((processed_data.count(), len(processed_data.columns)))\n",
    "\n",
    "# now we have 7 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEP : 3 Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of null row :  0\n"
     ]
    }
   ],
   "source": [
    "# checking null rows in original dataset\n",
    "print('Count of null row : ', data.filter(data.log.isNull()).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values cell :  94533\n"
     ]
    }
   ],
   "source": [
    "# checking null value for each columns in processed dataset\n",
    "isnull_rows_df = processed_data.filter(processed_data['host'].isNull()| \n",
    "                                       processed_data['timestamp'].isNull() | \n",
    "                                       processed_data['method'].isNull() |\n",
    "                                       processed_data['endpoint'].isNull() |\n",
    "                                       processed_data['status'].isNull() |\n",
    "                                       processed_data['content_size'].isNull()|\n",
    "                                       processed_data['protocol'].isNull())\n",
    "print('Missing values cell : ', isnull_rows_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0| 60679|       94529|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing col and sum \n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import sum as col_sum\n",
    "\n",
    "def count_null_cell(col_name):\n",
    "    return col_sum(col(col_name).isNull().cast('integer')).alias(col_name)\n",
    "\n",
    "# list of column expressions\n",
    "all_columns_exp = []\n",
    "for col_names in processed_data.columns:\n",
    "    all_columns_exp.append(count_null_cell(col_names))\n",
    "\n",
    "# converts the list of expressions to variable function arguments.\n",
    "processed_data.agg(*all_columns_exp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value for status code :  60679\n"
     ]
    }
   ],
   "source": [
    "# checking null value for status\n",
    "null_status = processed_data.filter(~data['log'].rlike(status_code_regx))\n",
    "print('Null value for status code : ', null_status.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|log                                                                                            |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|205.189.154.54 - - [01/Jul/1995:00:01:06 -0400] \"GET /cgi-bin/imagemap/countdown?99            |\n",
      "|remote27.compusmart.ab.ca - - [01/Jul/1995:00:01:53 -0400] \"GET /cgi-bin/imagemap/countdown?102|\n",
      "|onyx.southwind.net - - [01/Jul/1995:00:02:27 -0400] \"GET /cgi-bin/imagemap/countdown?103       |\n",
      "|gater3.sematech.org - - [01/Jul/1995:00:02:41 -0400] \"GET /cgi-bin/imagemap/countdown?99       |\n",
      "|onyx.southwind.net - - [01/Jul/1995:00:03:00 -0400] \"GET /cgi-bin/imagemap/countdown?102       |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing dataset for null status\n",
    "null_status_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records with null status removed :  3400934\n"
     ]
    }
   ],
   "source": [
    "# removing null status rows\n",
    "processed_data = processed_data[processed_data['status'].isNotNull()] \n",
    "print('Records with null status removed : ', processed_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     0|       33854|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing dataset after wrangling status codem\n",
    "processed_data.agg(*all_columns_exp).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value for content size :  33854\n"
     ]
    }
   ],
   "source": [
    "# checking null value for content size\n",
    "null_content_size = processed_data.filter(~data['log'].rlike(content_size_regx))\n",
    "print('Null value for content size : ', null_content_size.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------------+------+--------------------------------------------------------------------+--------+------+------------+\n",
      "|host                    |timestamp                 |method|endpoint                                                            |protocol|status|content_size|\n",
      "+------------------------+--------------------------+------+--------------------------------------------------------------------+--------+------+------------+\n",
      "|dd15-062.compuserve.com |01/Jul/1995:00:01:12 -0400|GET   |/news/sci.space.shuttle/archive/sci-space-shuttle-22-apr-1995-40.txt|HTTP/1.0|404   |null        |\n",
      "|dynip42.efn.org         |01/Jul/1995:00:02:14 -0400|GET   |/software                                                           |HTTP/1.0|302   |null        |\n",
      "|ix-or10-06.ix.netcom.com|01/Jul/1995:00:02:40 -0400|GET   |/software/winvn                                                     |HTTP/1.0|302   |null        |\n",
      "|ix-or10-06.ix.netcom.com|01/Jul/1995:00:03:24 -0400|GET   |/software                                                           |HTTP/1.0|302   |null        |\n",
      "|link097.txdirect.net    |01/Jul/1995:00:05:06 -0400|GET   |/shuttle                                                            |HTTP/1.0|302   |null        |\n",
      "+------------------------+--------------------------+------+--------------------------------------------------------------------+--------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing dataset for null status\n",
    "null_content_size.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after replacing it by 0 :  3400934\n"
     ]
    }
   ],
   "source": [
    "# filling null content size with 0 value\n",
    "processed_data = processed_data.na.fill({'content_size': 0})\n",
    "print('Records after replacing it by 0 : ', processed_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+------+--------+--------+------+------------+\n",
      "|host|timestamp|method|endpoint|protocol|status|content_size|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "|   0|        0|     0|       0|       0|     0|           0|\n",
      "+----+---------+------+--------+--------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing dataset after wrangling content_size\n",
    "processed_data.agg(*all_columns_exp).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : Now after data cleaning there is no missing values in our processed dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Changing timestamp field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- host: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- endpoint: string (nullable = true)\n",
      " |-- protocol: string (nullable = true)\n",
      " |-- status: integer (nullable = true)\n",
      " |-- content_size: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing schema\n",
    "processed_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE : Here timestamp column is of string type, so we have convert it into timestamp type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
